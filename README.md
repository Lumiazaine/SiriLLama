<div align = "center">
<h1>
    <img src = "https://github.com/0ssamaak0/SiriLLama/blob/main/assets/icon.png?raw=true" width = 200 height = 200>
<br>

</h1>

<h3>
Siri LLama
</h3>
</div>

Siri LLama es un atajo (shortcut) de Apple que permite acceder a modelos LLMs que se ejecutan localmente, ya sea a trav√©s de Siri o de la interfaz del atajo, en cualquier dispositivo Apple conectado a la misma red del servidor. Utiliza Langchain ü¶úüîó y es compatible con modelos de c√≥digo abierto tanto de [Ollama](https://ollama.com/) ü¶ô u otros.

# Descarga el [Shortcut](https://www.icloud.com/shortcuts/fd032a4e75cc4d81a6f9a742053d4c18)

# Comienzo
## Requisitos

1. Descargar [Git](https://git-scm.com/downloads/), puedes descargar el ejecutable o hacerlo por la terminal.


```bash
winget install --id=Git.Git  -e
```

2. Una vez descargado Git, descargamos el repositorio y nos dirigimos a √©l.

```bash
git clone https://github.com/Lumiazaine/SiriLLama.git #Descarga el repositorio de github
cd SiriLLama #Nos dirigimos a la ruta
```

### Instalaci√≥n de Ollamaü¶ô
1. Instalar [Ollama](https://ollama.com/) en el servidor.

2. Descarga los modelos que quieras usar, en este caso
```bash
ollama run gemma2 # LLM principal para tareas
ollama run llava # Para tareas como reconocimiento de im√°genes
ollama pull nextfire/paraphrase-multilingual-minilm # modelo de Embedding
```

3. En `config.py` establece `OLLAMA_CHAT`, `OLLAMA_VISUAL_CHAT`, y `OLLAMA_EMBEDDINGS_MODEL` Los modelos que has descargado en ollama y quieres utilizar.

### Instalaci√≥n y despliegue con Docker.  üê≥ 

1. Instalar [Docker](https://www.docker.com/products/docker-desktop/) en el servidor.

2. Desde la carpeta Sirillama ejecutamos los siguientes comentos

```bash
docker build -t sirillama .
docker run -d --name sirillamabeta --restart always -p 5001:5001 sirillama
```

Desde el log deber√≠a aparecerte algo similar

```bash
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.1.134:5001
Press CTRL+C to quit
```
## Ajustes
En `confing.py` establece `MEMORY_SIZE` (Los mensajes anteriores a recordar) and `ANSWER_SIZE_WORDS` (Cuantas palabras quiere que contenga la respuesta)

## Ejecutando SiriLlama en tu dispositivo Apple üü£ü¶ô

4. En tu dispositivo Apple, descarga el siguiente atajo [aqu√≠](https://www.icloud.com/shortcuts/fd032a4e75cc4d81a6f9a742053d4c18).
Ten en cuenta que, para ‚Äúhablar‚Äù con √©l, debes ejecutar el atajo a trav√©s de Siri; de lo contrario, se te solicitar√° que escribas el texto.

5. Ejecuta el atajo a trav√©s de Siri o de la interfaz del propio atajo. La primera vez que lo ejecutes, se te pedir√° que introduzcas tu [direcci√≥n IP](https://stackoverflow.com/a/15864222) y el n√∫mero de puerto que aparece en la terminal.

En el ejemplo anterior, la direcci√≥n IP es `192.168.1.134` y el puerto es `5001` (Es el puerto predeterminado, puedes cambiarlo si fuera necesario desde main.py).

6. Si est√°s usando Siri para interactuar con el atajo, decir ‚ÄúAdi√≥s‚Äù detendr√° a Siri.


# Otros modelos LLM ü§ñü§ñ
Supuestamente, SiriLLama deber√≠a funcionar con cualquier LLM, incluidos OpenAI, Claude, etc. Pero aseg√∫rate primero de haber instalado los paquetes correspondientes de Langchain y de configurar los modelos en... `config.py`

# SiriLLama en redes p√∫blicas. üåé
- Es posible ejecutar Siri ollama sin estar en la red local a trav√©s de VPN como [Wireguard](https://www.wireguard.com/) o similares.

# Tips üí°üí°
- Para usar la funci√≥n multimodal, solo es posible con im√°genes que no est√©n en formato HEIF. Puedes cambiar esto en la configuraci√≥n de tu c√°mara (no afectar√° a tus fotos existentes). En la secci√≥n de formatos, elige ‚ÄúM√°s compatible‚Äù ¬°y listo!
